# Monitring-Pipeline

Project Name: Real-Time Price Monitoring Pipeline with Snowflake, EMR, and Redshift

üåê Use Case:
Scrape product prices from websites in real time, stream data via AWS, clean/process it on EMR, store in S3, and push it to both Redshift and Snowflake for BI dashboards and analytics.

üß± Revised Architecture:


```
[Lambda Web Scraper + EventBridge]
              ‚Üì
     [AWS Kinesis Data Streams]
              ‚Üì
           [AWS Lambda]
              ‚Üì
          [S3 Raw Zone]
              ‚îÇ
              ‚îî‚îÄ‚îÄ‚ñ∂ [AWS Glue Crawler]
              ‚Üì
[EMR Spark Job for Cleaning/Transforming]
              ‚Üì
        [S3 Processed Zone]
             ‚îÇ       ‚îÇ
             ‚ñº       ‚ñº
        [Redshift]  [Snowflake]
             ‚ñº           ‚ñº
  [Amazon QuickSight]   [Snowflake Snowsight / Tableau / Power BI]
```







## üîÅ End-to-End Data Pipeline Steps

---

### üîπ Step 1: Web Scraper (Lambda + EventBridge)
- Use a Python AWS Lambda function with `requests` and `BeautifulSoup`.
- Schedule it to run every 5 minutes using Amazon EventBridge (CloudWatch Events).
- Send scraped product price data to **Amazon Kinesis Data Streams**.

---

### üîπ Step 2: Real-Time Ingestion (Kinesis)
- **Kinesis** captures real-time product price events.
- Triggers another **Lambda** function to store incoming events into **Amazon S3 (Raw Zone)**.

---

### üîπ Step 3: Raw Data Cataloging (AWS Glue Crawler)
- **AWS Glue Crawler** scans the S3 Raw Zone and catalogs the data in the AWS Glue Data Catalog.
- *(Optional)*: Query the raw data directly using **Amazon Athena**.

---

### üîπ Step 4: Data Processing on Amazon EMR
- Use **Apache Spark** (PySpark) running on **Amazon EMR**.
- Read from the **S3 Raw Zone**.
- Clean and transform data:
  - Cast `price` to float
  - Deduplicate records
- Write the cleaned data to the **S3 Processed Zone** in formats like **Parquet**.

---

### üîπ Step 5: Load into Amazon Redshift
- Use the **Redshift COPY command** or **AWS Glue Job** to load data.
- You can:
  - Load into an internal Redshift table
  - Or use **Redshift Spectrum** with external tables pointing to S3

---

### üîπ Step 6: Load into Snowflake
- Use **Snowpipe** for near real-time loading from S3.
- Alternatively, use **Snowflake Tasks** to run scheduled batch jobs.
- Process involves:
  - Reading from S3 Processed Zone
  - Using `STAGE` + `FILE FORMAT` + `COPY INTO` commands

---

### üîπ Step 7: Visualization
- Use **Amazon QuickSight** for dashboards built on Redshift.
- For Snowflake, use:
  - **Snowflake Snowsight**
  - Or BI tools like **Tableau** or **Power BI**

---






## üß† Tools & Skills You‚Äôll Demonstrate

| **Area**               | **Tool/Service**                                      |
|------------------------|-------------------------------------------------------|
| Web Scraping           | Python, BeautifulSoup                                 |
| Real-Time Ingestion    | AWS Kinesis, Lambda                                   |
| Data Lake              | S3, Glue Crawler, Athena                              |
| Data Processing        | Amazon EMR (PySpark)                                  |
| Data Warehouse         | Redshift, Snowflake                                   |
| BI & Dashboards        | QuickSight, Snowflake Snowsight                       |
| Automation             | EventBridge, Snowpipe, Lambda Triggers                |


